// src/costing/index.js
import chalk from 'chalk';
import { get_encoding } from 'tiktoken';
import * as config from '../config/index.js';

// --- Initialize Tokenizer (using tiktoken for estimation) ---
let tokenizer;
try {
    // Using 'cl100k_base' which is common for GPT-3.5/4, as a proxy for Gemini.
    // Acknowledge this is an ESTIMATE.
    tokenizer = get_encoding("cl100k_base");
    console.log(chalk.yellow("Tokenizer loaded (using tiktoken 'cl100k_base' for estimation). Cost estimates may not be exact for Gemini."));
} catch (e) {
    console.error(chalk.red("ðŸš¨ Failed to load tokenizer. Cost estimation disabled."), e);
    tokenizer = null; // Disable estimation if tokenizer fails
}

/**
 * Estimates the number of tokens in a string using tiktoken.
 * @param {string} text The string to count tokens for.
 * @returns {number} Estimated token count, or 0 if tokenizer failed.
 */
export function estimateTokens(text) {
    if (!tokenizer || typeof text !== 'string') {
        return 0;
    }
    try {
        return tokenizer.encode(text).length;
    } catch (e) {
        console.warn(chalk.yellow("Warning: Token estimation failed for a string."), e);
        return 0; // Return 0 on error
    }
}

/**
 * Estimates the token count for the messages array structure.
 * Adds a small overhead per message based on OpenAI's approximation.
 * @param {Array<Object>} messages The messages array.
 * @returns {number} Estimated token count.
 */
export function estimateMessagesTokens(messages) {
    if (!tokenizer || !Array.isArray(messages)) return 0;
    let numTokens = 0;
    messages.forEach(message => {
        numTokens += 4; // Base overhead per message (role, content separation etc)
        for (const [key, value] of Object.entries(message)) {
            if (typeof value === 'string') {
                 numTokens += estimateTokens(value);
            } else if (key === 'tool_calls' && Array.isArray(value)) {
                // Estimate tokens for tool call requests generated by the model
                value.forEach(toolCall => {
                    if(toolCall.function?.name) numTokens += estimateTokens(toolCall.function.name);
                    if(toolCall.function?.arguments) numTokens += estimateTokens(toolCall.function.arguments);
                });
            }
            // Note: We don't add key tokens explicitly here, tiktoken on stringified
            // content is usually sufficient for estimation.
        }
    });
    numTokens += 2; // Final assistant message priming
    return numTokens;
}


/**
 * Calculates the estimated cost of a single API call.
 * @param {number} inputTokens Estimated input tokens.
 * @param {number} outputTokens Estimated output tokens.
 * @param {string} modelName The name of the model used.
 * @returns {{cost: number, currency: string}} Estimated cost and currency, or {cost: 0, currency: 'USD'} if pricing not found.
 */
export function calculateApiCallCost(inputTokens, outputTokens, modelName = config.MODEL) {
    const pricing = config.MODEL_PRICING[modelName];
    if (!pricing) {
        console.warn(chalk.yellow(`Warning: Pricing info not found for model "${modelName}". Cost calculation skipped.`));
        return { cost: 0, currency: 'USD' };
    }

    const inputCost = (inputTokens / 1_000_000) * pricing.inputCostPerMillionTokens;
    const outputCost = (outputTokens / 1_000_000) * pricing.outputCostPerMillionTokens;
    const totalCost = inputCost + outputCost;

    return { cost: totalCost, currency: pricing.currency };
}

// Function to get the active tokenizer (or null) if needed elsewhere
export function getTokenizer() {
    return tokenizer;
}